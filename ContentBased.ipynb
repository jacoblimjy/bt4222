{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# If 'courses' might be a slice of another DataFrame, explicitly create a copy to work on:\n",
    "courses = pd.read_csv(\"./CleanedDataset/final_courses.csv\")\n",
    "\n",
    "courses = courses.drop_duplicates(subset='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Quang\n",
      "[nltk_data]     Anh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Quang\n",
      "[nltk_data]     Anh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Quang\n",
      "[nltk_data]     Anh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "additional_stopwords = ['learn', 'course', 'students', 'introduction', 'understand', 'overview', \n",
    "    'concepts', 'designed', 'including', 'using', 'used', 'provides', \n",
    "    'students', 'aimed', 'teach', 'taught', 'include', 'includes', 'including', \n",
    "    'understanding', 'module', 'modules', 'cover', 'covers', 'covered', \n",
    "    'introduce', 'introduces', 'introduced', 'overview', 'knowledge', 'skills', \n",
    "    'learning', 'lecture', 'lectures', 'topic', 'topics', 'area', 'areas', \n",
    "    'focus', 'focuses', 'focused', 'approach', 'approaches', 'principles', \n",
    "    'principle', 'content', 'contents', 'key', 'features', 'feature', \n",
    "    'element', 'elements', 'basis', 'basic', 'basics', 'foundation', 'foundations',\n",
    "    'beginner', 'intermediate', 'advanced', 'level', 'levels', 'intended',\n",
    "    'objective', 'objectives', 'goal', 'goals', 'outcome', 'outcomes', 'aim', 'aims',\n",
    "    'gain', 'gains', 'apply', 'effective', 'efficient', 'specialize', 'specialization',\n",
    "    'specialise', 'certificate', 'certification', 'you', 'will', 'need',\n",
    "    'want', 'have', 'able', 'well', 'one', 'two', 'three', 'first', 'second',\n",
    "    'third', 'success', 'successful', 'sucessfully', 'like', 'many' , 'much', 'also', 'use', 'uses', 'used',\n",
    "    'work', 'works', 'working', 'workshop', 'workshops', 'provide', 'provides', 'provided']\n",
    "\n",
    "all_stop_words = stopwords.words('english') + additional_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Preprocessing function with N-gram consideration\n",
    "def preprocess_text(text, lemmatizer=WordNetLemmatizer(), stop_words=all_stop_words):\n",
    "    text = text.lower()  # Lowercasing\n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    alphabetic_tokens = [token for token in tokens if re.match(\"^[a-zA-Z]+$\", token)] # check for alphabetic tokens\n",
    "    filtered_tokens = [word for word in alphabetic_tokens if word not in stop_words]  # Remove stopwords\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]  # Lemmatization\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, safely apply the preprocessing and assign to the 'processed_description' column\n",
    "courses['processed_description'] = courses['description'].apply(preprocess_text)\n",
    "courses['processed_skills'] = courses['skills'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Quang Anh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def get_bert_embeddings(texts):\n",
    "    # Tokenize the text\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    \n",
    "    # Move encoded input to the correct device\n",
    "    encoded_input = encoded_input.to(device)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    \n",
    "    # Retrieve the embeddings for the [CLS] token (first token) as sentence embeddings\n",
    "    embeddings = output.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "    \n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_embeddings = get_bert_embeddings(courses['processed_description'].tolist())\n",
    "skills_embeddings = get_bert_embeddings(courses['processed_skills'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses['description_embeddings'] = description_embeddings.tolist()\n",
    "courses['skills_embeddings'] = skills_embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Assuming courses['description_embeddings'] is a list of embeddings\n",
    "embeddings_array = np.array(courses['description_embeddings'].tolist())\n",
    "\n",
    "# Initialize PCA, here we are reducing to 128 components\n",
    "pca = PCA(n_components=128)\n",
    "reduced_embeddings = pca.fit_transform(embeddings_array)\n",
    "\n",
    "# Store the reduced embeddings back into the DataFrame\n",
    "courses['reduced_embeddings'] = list(reduced_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Assuming courses['description_embeddings'] is a list of embeddings\n",
    "embeddings_array = np.array(courses['skills_embeddings'].tolist())\n",
    "\n",
    "# Initialize PCA, here we are reducing to 128 components\n",
    "pca = PCA(n_components=128)\n",
    "reduced_embeddings = pca.fit_transform(embeddings_array)\n",
    "\n",
    "# Store the reduced embeddings back into the DataFrame\n",
    "courses['skills_embeddings'] = list(reduced_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming `courses_embeddings` and `skills_embeddings` are NumPy arrays\n",
    "# If they are lists, first convert them: np.array(list_of_arrays)\n",
    "description_similarity = cosine_similarity(description_embeddings)\n",
    "skills_similarity = cosine_similarity(skills_embeddings)\n",
    "\n",
    "# Combine the matrices, here we're using a simple average\n",
    "combined_similarity = description_similarity * 0.3 + skills_similarity * 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Course Indices: [ 70  71  57  32 214]\n",
      "Scores: [0.92423487 0.9199227  0.91198486 0.9068167  0.8935572 ]\n",
      "Rcommendations for 122: natural language processing with sequence models\n",
      "\n",
      "\n",
      "natural language processing in tensorflow\n",
      "natural language processing with probabilistic models\n",
      "convolutional neural networks\n",
      "natural language processing with classification and vector spaces\n",
      "introduction to machine learning\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "def recommend_course(course_index, combined_similarity, top_n=5):\n",
    "    # Scores are the similarity of the selected course to all others\n",
    "    scores = combined_similarity[course_index]\n",
    "\n",
    "    # Sort the scores in descending order while keeping track of the index\n",
    "    sorted_indices = scores.argsort()[::-1]\n",
    "    \n",
    "    # Exclude the course itself (highest score), and get the top_n recommendations\n",
    "    top_course_indices = sorted_indices[1:top_n+1]\n",
    "\n",
    "    # The scores for the top recommended courses\n",
    "    top_scores = scores[top_course_indices]\n",
    "    \n",
    "    # You can then use `top_course_indices` to fetch the course names or IDs if you have them\n",
    "    # For example, assuming you have a DataFrame `courses` with a course ID or name:\n",
    "    # recommended_courses = courses.iloc[top_course_indices]\n",
    "    \n",
    "    return top_course_indices, top_scores\n",
    "\n",
    "# Example: Recommend courses similar to the course at index 0\n",
    "course_index = random.randint(0, len(courses)-1)\n",
    "top_n = 5\n",
    "recommended_indices, recommended_scores = recommend_course(course_index, combined_similarity, top_n)\n",
    "\n",
    "print(\"Recommended Course Indices:\", recommended_indices)\n",
    "print(\"Scores:\", recommended_scores)\n",
    "\n",
    "print(f\"Rcommendations for {course_index}: {courses['name'][course_index]}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in recommended_indices:\n",
    "    print(courses['name'][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
