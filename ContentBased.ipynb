{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# If 'courses' might be a slice of another DataFrame, explicitly create a copy to work on:\n",
    "courses = pd.read_csv(\"./CleanedDataset/final_courses.csv\")\n",
    "\n",
    "courses = courses.drop_duplicates(subset='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Quang\n",
      "[nltk_data]     Anh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Quang\n",
      "[nltk_data]     Anh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Quang\n",
      "[nltk_data]     Anh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "additional_stopwords = ['learn', 'course', 'students', 'introduction', 'understand', 'overview', \n",
    "    'concepts', 'designed', 'including', 'using', 'used', 'provides', \n",
    "    'students', 'aimed', 'teach', 'taught', 'include', 'includes', 'including', \n",
    "    'understanding', 'module', 'modules', 'cover', 'covers', 'covered', \n",
    "    'introduce', 'introduces', 'introduced', 'overview', 'knowledge', 'skills', \n",
    "    'learning', 'lecture', 'lectures', 'topic', 'topics', 'area', 'areas', \n",
    "    'focus', 'focuses', 'focused', 'approach', 'approaches', 'principles', \n",
    "    'principle', 'content', 'contents', 'key', 'features', 'feature', \n",
    "    'element', 'elements', 'basis', 'basic', 'basics', 'foundation', 'foundations',\n",
    "    'beginner', 'intermediate', 'advanced', 'level', 'levels', 'intended',\n",
    "    'objective', 'objectives', 'goal', 'goals', 'outcome', 'outcomes', 'aim', 'aims',\n",
    "    'gain', 'gains', 'apply', 'effective', 'efficient', 'specialize', 'specialization',\n",
    "    'specialise', 'certificate', 'certification', 'you', 'will', 'need',\n",
    "    'want', 'have', 'able', 'well', 'one', 'two', 'three', 'first', 'second',\n",
    "    'third', 'success', 'successful', 'sucessfully', 'like', 'many' , 'much', 'also', 'use', 'uses', 'used',\n",
    "    'work', 'works', 'working', 'workshop', 'workshops', 'provide', 'provides', 'provided']\n",
    "\n",
    "all_stop_words = stopwords.words('english') + additional_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Preprocessing function with N-gram consideration\n",
    "def preprocess_text(text, lemmatizer=WordNetLemmatizer(), stop_words=all_stop_words):\n",
    "    text = text.lower()  # Lowercasing\n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    alphabetic_tokens = [token for token in tokens if re.match(\"^[a-zA-Z]+$\", token)] # check for alphabetic tokens\n",
    "    filtered_tokens = [word for word in alphabetic_tokens if word not in stop_words]  # Remove stopwords\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]  # Lemmatization\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, safely apply the preprocessing and assign to the 'processed_description' column\n",
    "courses['processed_description'] = courses['description'].apply(preprocess_text)\n",
    "courses['processed_skills'] = courses['skills'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def get_bert_embeddings(texts):\n",
    "    # Tokenize the text\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    \n",
    "    # Move encoded input to the correct device\n",
    "    encoded_input = encoded_input.to(device)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_input)\n",
    "    \n",
    "    # Retrieve the embeddings for the [CLS] token (first token) as sentence embeddings\n",
    "    embeddings = output.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "    \n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_embeddings = get_bert_embeddings(courses['processed_description'].tolist())\n",
    "skills_embeddings = get_bert_embeddings(courses['processed_skills'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses['description_embeddings'] = description_embeddings.tolist()\n",
    "courses['skills_embeddings'] = skills_embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming `courses_embeddings` and `skills_embeddings` are NumPy arrays\n",
    "# If they are lists, first convert them: np.array(list_of_arrays)\n",
    "description_similarity = cosine_similarity(description_embeddings)\n",
    "skills_similarity = cosine_similarity(skills_embeddings)\n",
    "\n",
    "# Combine the matrices, here we're using a simple average\n",
    "combined_similarity = description_similarity * 0.3 + skills_similarity * 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Course Indices: [120  86 179  21 158]\n",
      "Scores: [0.91410613 0.9114789  0.9103676  0.9049592  0.90307534]\n",
      "Rcommendations for: introduction to tensorflow for artificial intelligence, machine learning, and deep learning\n",
      "\n",
      "\n",
      "introduction to spreadsheets and models\n",
      "fundamentals of gis\n",
      "cybersecurity compliance framework & system administration\n",
      "computational thinking for problem solving\n",
      "essential epidemiologic tools for public health practice\n"
     ]
    }
   ],
   "source": [
    "def recommend_course(course_index, combined_similarity, top_n=5):\n",
    "    # Scores are the similarity of the selected course to all others\n",
    "    scores = combined_similarity[course_index]\n",
    "\n",
    "    # Sort the scores in descending order while keeping track of the index\n",
    "    sorted_indices = scores.argsort()[::-1]\n",
    "    \n",
    "    # Exclude the course itself (highest score), and get the top_n recommendations\n",
    "    top_course_indices = sorted_indices[1:top_n+1]\n",
    "\n",
    "    # The scores for the top recommended courses\n",
    "    top_scores = scores[top_course_indices]\n",
    "    \n",
    "    # You can then use `top_course_indices` to fetch the course names or IDs if you have them\n",
    "    # For example, assuming you have a DataFrame `courses` with a course ID or name:\n",
    "    # recommended_courses = courses.iloc[top_course_indices]\n",
    "    \n",
    "    return top_course_indices, top_scores\n",
    "\n",
    "# Example: Recommend courses similar to the course at index 0\n",
    "course_index = random.randint(0, len(courses)-1)\n",
    "top_n = 5\n",
    "recommended_indices, recommended_scores = recommend_course(course_index, combined_similarity, top_n)\n",
    "\n",
    "print(\"Recommended Course Indices:\", recommended_indices)\n",
    "print(\"Scores:\", recommended_scores)\n",
    "\n",
    "print(f\"Rcommendations for: {courses['name'][course_index]}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in recommended_indices:\n",
    "    print(courses['name'][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
