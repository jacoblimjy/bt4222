{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "courses_data = pd.read_csv(\"final_users_courses.csv\")\n",
    "job_data = pd.read_csv(\"final_jobs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_abs_value_courses = courses_data['Demeaned Rating'].abs().max()\n",
    "courses_data['Normalised Demeaned Rating'] = courses_data['Demeaned Rating'] / max_abs_value_courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create an instance of the SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Apply the sentiment analyzer to the \"reviews\" column\n",
    "courses_data['Sentiment_Score'] = courses_data['Review'].apply(lambda x: sia.polarity_scores(str(x))['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-course interaction matrix\n",
    "grouped_data_train = courses_data.groupby('Reviewer')\n",
    "user_course_interaction_train = {}\n",
    "\n",
    "for user, user_data in grouped_data_train:\n",
    "    interactions = {}\n",
    "    for _, row in user_data.iterrows():\n",
    "        course_name = row['Course Name']\n",
    "        rating = row['Normalised Demeaned Rating']\n",
    "        sentiment_score = row['Sentiment_Score']\n",
    "        interactions[course_name] = 0.4*rating + 0.6*sentiment_score\n",
    "    user_course_interaction_train[user] = interactions\n",
    "\n",
    "user_course_matrix = pd.DataFrame.from_dict(user_course_interaction_train, orient=\"index\")\n",
    "user_course_matrix.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "courses_data_no_duplicates = courses_data.drop_duplicates(subset=['Course Name', 'Reviewer'])\n",
    "\n",
    "# Load the data into Surprise format\n",
    "reader = Reader(rating_scale=(-1, 1))  # Assuming your ratings range from -1 to 1\n",
    "courses_data_surprise = Dataset.load_from_df(courses_data[['Reviewer', 'Course Name', 'Normalised Demeaned Rating']], reader)\n",
    "\n",
    "#Train and Test split\n",
    "train, test = train_test_split(courses_data_surprise, test_size=0.1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the item-based collaborative filtering model\n",
    "model = KNNBasic(sim_options={'user_based': False, 'name':'pearson'}, k=10)\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.test(test)\n",
    "\n",
    "# Group the predictions by user\n",
    "user_predictions = defaultdict(list)\n",
    "for uid, iid, true_r, est, _ in predictions:\n",
    "    user_predictions[uid].append((iid, est))\n",
    "\n",
    "# Sort the predictions for each user by the estimated rating in descending order\n",
    "# and remove duplicates by converting the list to a set and back to a list\n",
    "for uid, user_preds in user_predictions.items():\n",
    "    user_preds = list(set(user_preds))  # remove duplicates\n",
    "    user_preds.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top 5 course recommendations for each user\n",
    "top_k_recommendations = {}\n",
    "for uid, user_preds in user_predictions.items():\n",
    "    top_k_recommendations[uid] = [iid for iid, _ in user_preds[:5]]\n",
    "\n",
    "# Initialize an empty list to store the recommendations\n",
    "recommendations = []\n",
    "\n",
    "# Get the top 5 course recommendations for each user\n",
    "for uid, user_preds in user_predictions.items():\n",
    "    for iid, score in user_preds[:5]:\n",
    "        recommendations.append((uid, iid, score))\n",
    "\n",
    "# Convert the list of recommendations to a DataFrame\n",
    "df_recommendations = pd.DataFrame(recommendations, columns=['Reviewers', 'Course Name', 'Score'])\n",
    "\n",
    "# Sort the recommendations by User and Score in descending order\n",
    "df_recommendations = df_recommendations.sort_values(by=['Reviewers', 'Score'], ascending=[True, False])\n",
    "\n",
    "df_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the score matrix\n",
    "score_matrix = defaultdict(dict)\n",
    "\n",
    "# Populate the score matrix with predictions\n",
    "for uid, iid, true_r, est, _ in predictions:\n",
    "    score_matrix[uid][iid] = est\n",
    "\n",
    "# Fill in missing course scores for each user\n",
    "for uid in score_matrix:\n",
    "    all_course_ids = set(score_matrix[uid]) | set(train['Course Name'].unique())\n",
    "    for course_id in all_course_ids:\n",
    "        if course_id not in score_matrix[uid]:\n",
    "            score_matrix[uid][course_id] = None\n",
    "\n",
    "# Convert the score matrix to a DataFrame\n",
    "df_score_matrix = pd.DataFrame(score_matrix)\n",
    "\n",
    "# Transpose the DataFrame to have users as rows and courses as columns\n",
    "df_score_matrix = df_score_matrix.T\n",
    "\n",
    "# Sort the DataFrame by user ID\n",
    "df_score_matrix = df_score_matrix.sort_index()\n",
    "\n",
    "# Fill in missing values with 0\n",
    "df_score_matrix.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import RandomizedSearchCV\n",
    "from surprise import KNNBasic\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'k': [10, 20], 'sim_options': {'name': ['pearson']}}\n",
    "\n",
    "# Initialize the GridSearchCV object with FCP as the evaluation metric\n",
    "grid_search = RandomizedSearchCV(KNNBasic, param_grid, measures=['fcp'], cv=3, n_iter=2)\n",
    "\n",
    "# Load the data into a surprise Dataset object\n",
    "entire_data = Dataset.load_from_df(courses_data[['Reviewer', 'Course Name', 'Normalised Demeaned Rating']], reader)\n",
    "\n",
    "# Fit the GridSearchCV object\n",
    "grid_search.fit(entire_data)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params['fcp']\n",
    "print(\"Best hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "# Value of k (Top 5 Recommendations)\n",
    "k = 5\n",
    "\n",
    "# Calculate FCP\n",
    "fcp = accuracy.fcp(predictions, verbose=True)\n",
    "\n",
    "print(\"Precision Score at k =\", 5, \":\", fcp)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = accuracy.mae(predictions, verbose=True)\n",
    "\n",
    "print(\"MAE Score at k =\", 5, \":\", mae)\n",
    "\n",
    "# Calculate MSE\n",
    "mse = accuracy.mse(predictions, verbose=True)\n",
    "\n",
    "print(\"MSE Score at k =\", 5, \":\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
