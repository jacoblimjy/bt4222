{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/Users/Quang Anh/Desktop/job_skills.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keywords'] = data['job_link'].str.extract('view/(.*)')[0].str.split('-at-').str[0].str.split('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('job_link', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\n",
    "    'data', 'finance', 'financial', 'business',\n",
    "    'accountant', 'analyst', 'architect', 'auditor', 'barista',\n",
    "    'chef', 'consultant', 'conductor' 'designer', 'developer', 'economist',\n",
    "    'editor', 'engineer', 'instructor', 'operator', 'journalist',\n",
    "    'lawyer', 'manager', 'marketer', 'nurse', 'pharmacist',\n",
    "    'photographer', 'planner', 'programmer', 'psychologist', 'recruiter',\n",
    "    'scientist', 'surgeon', 'therapist', 'trainer', 'writer',\n",
    "    'actuary', 'broker', 'butler', 'cashier', 'custodian',\n",
    "    'dietitian', 'electrician', 'farmer', 'gardener', 'hygienist',\n",
    "    'illustrator', 'judge', 'librarian', 'mason', 'navigator',\n",
    "    'optometrist', 'paramedic', 'quartermaster', 'ranger', 'sculptor',\n",
    "    'tailor', 'underwriter', 'valet', 'welder', 'xylographer',\n",
    "    'yeoman', 'zoologist'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the DataFrame\n",
    "data['filtered_entries'] = data['keywords'].apply(lambda x: [word for word in x if word in titles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with no matches (empty lists)\n",
    "filtered_jobs = data[data['filtered_entries'].map(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('manager', 257130), ('nurse', 113513), ('engineer', 81771), ('analyst', 34740), ('consultant', 20781), ('accountant', 12337), ('scientist', 11449), ('therapist', 11118), ('instructor', 9623), ('architect', 9465), ('developer', 8563), ('chef', 8526), ('planner', 6737), ('trainer', 5981), ('cashier', 5032), ('operator', 3933), ('psychologist', 3743), ('surgeon', 3636), ('auditor', 2963), ('writer', 2635), ('recruiter', 2053), ('pharmacist', 1805), ('electrician', 1621), ('underwriter', 1364), ('editor', 1111), ('programmer', 1004), ('lawyer', 925), ('barista', 816), ('navigator', 675), ('broker', 512), ('custodian', 452), ('welder', 443), ('hygienist', 322), ('photographer', 270), ('dietitian', 248), ('paramedic', 242), ('optometrist', 225), ('actuary', 148), ('gardener', 121), ('librarian', 116), ('economist', 102), ('valet', 99), ('mason', 84), ('butler', 71), ('marketer', 60), ('journalist', 46), ('judge', 44), ('ranger', 35), ('illustrator', 26), ('tailor', 12), ('farmer', 11), ('zoologist', 2), ('sculptor', 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# EDA Step 1: Frequency Distribution of job titles\n",
    "job_title_counts = Counter([job for entry in filtered_jobs['filtered_entries'] for job in entry])\n",
    "\n",
    "# EDA Results\n",
    "job_title_counts\n",
    "\n",
    "most_common_titles = job_title_counts.most_common()  # This will sort job titles by frequency in descending order\n",
    "print(most_common_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_1 = pd.read_csv(\"C:/Users/Quang Anh/Desktop/CourseraDataset-Clean.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_1 = desc_1.loc[(desc_1['What you will learn'].notna()) & \n",
    "                               (desc_1['What you will learn'] != 'Not specified')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_1 = desc_1[['Course Title', 'What you will learn']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_2 = pd.read_csv(\"C:/Users/Quang Anh/Downloads/Telegram Desktop/coursera_courses (2).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_2 = desc_2.loc[(desc_2['course_description'].notna())]\n",
    "desc_2 = desc_2[['course_title', 'course_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_3 = pd.read_csv(\"C:/Users/Quang Anh/Downloads/coursera_course_dataset_v3.csv\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_3 = desc_3[['Title', 'course_description']]\n",
    "desc_3 = desc_3.loc[(desc_3['course_description'].notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 402 entries, 0 to 617\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Title               402 non-null    object\n",
      " 1   course_description  402 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 9.4+ KB\n"
     ]
    }
   ],
   "source": [
    "desc_3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses_data_cleaned = pd.read_csv(\"C:/Users/Quang Anh/Downloads/Telegram Desktop/final_courses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>reviewers</th>\n",
       "      <th>rating</th>\n",
       "      <th>name</th>\n",
       "      <th>institution</th>\n",
       "      <th>Overall Ratings</th>\n",
       "      <th>Level</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Num of Reviews</th>\n",
       "      <th>Skill gain</th>\n",
       "      <th>Instructor</th>\n",
       "      <th>Demeaned Rating</th>\n",
       "      <th>Fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The course was very useful, and it covered all...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Structuring Machine Learning Projects</td>\n",
       "      <td>DeepLearning.AI</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Beginner level</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>Decision-Making, Machine Learning, Deep Learni...</td>\n",
       "      <td>Younes Bensouda Mourri', 'Kian Katanforoosh', ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>Introduction to Data Analysis Using Excel</td>\n",
       "      <td>Rice University</td>\n",
       "      <td>4.7</td>\n",
       "      <td>None</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>Lookup Table, Data Analysis, Microsoft Excel, ...</td>\n",
       "      <td>Sharad Borle</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very good refresher for Microsoft Excel</td>\n",
       "      <td>1017</td>\n",
       "      <td>5</td>\n",
       "      <td>Excel Skills for Business: Essentials</td>\n",
       "      <td>Macquarie University</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Beginner level</td>\n",
       "      <td>26</td>\n",
       "      <td>47</td>\n",
       "      <td>Graphs, Spreadsheet, Microsoft Excel, Chart</td>\n",
       "      <td>Nicky Bull', 'Professor Yvonne Breyer', 'Dr Pr...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I like the week 3 and week 4. Help me to revie...</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Science Math Skills</td>\n",
       "      <td>Duke University</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Beginner level</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>\"Bayes Theorem\", Bayesian Probability, Probabi...</td>\n",
       "      <td>Paul Bendich', 'Daniel Egger</td>\n",
       "      <td>0.25</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awesome!!!!!</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>Using Python to Access Web Data</td>\n",
       "      <td>University of Michigan</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Beginner level</td>\n",
       "      <td>19</td>\n",
       "      <td>43</td>\n",
       "      <td>Json, Xml, Python Programming, Web Scraping</td>\n",
       "      <td>Charles Russell Severance</td>\n",
       "      <td>0.25</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews reviewers  rating  \\\n",
       "0  The course was very useful, and it covered all...         1       5   \n",
       "1                                                ...         7       5   \n",
       "2            Very good refresher for Microsoft Excel      1017       5   \n",
       "3  I like the week 3 and week 4. Help me to revie...      2020       5   \n",
       "4                                       awesome!!!!!      2020       5   \n",
       "\n",
       "                                        name             institution  \\\n",
       "0      Structuring Machine Learning Projects         DeepLearning.AI   \n",
       "1  Introduction to Data Analysis Using Excel         Rice University   \n",
       "2      Excel Skills for Business: Essentials    Macquarie University   \n",
       "3                   Data Science Math Skills         Duke University   \n",
       "4            Using Python to Access Web Data  University of Michigan   \n",
       "\n",
       "   Overall Ratings           Level  Duration  Num of Reviews  \\\n",
       "0              4.8  Beginner level         6              49   \n",
       "1              4.7            None        24              10   \n",
       "2              4.9  Beginner level        26              47   \n",
       "3              4.5  Beginner level        13              11   \n",
       "4              4.8  Beginner level        19              43   \n",
       "\n",
       "                                          Skill gain  \\\n",
       "0  Decision-Making, Machine Learning, Deep Learni...   \n",
       "1  Lookup Table, Data Analysis, Microsoft Excel, ...   \n",
       "2        Graphs, Spreadsheet, Microsoft Excel, Chart   \n",
       "3  \"Bayes Theorem\", Bayesian Probability, Probabi...   \n",
       "4        Json, Xml, Python Programming, Web Scraping   \n",
       "\n",
       "                                          Instructor  Demeaned Rating  Fee  \n",
       "0  Younes Bensouda Mourri', 'Kian Katanforoosh', ...             0.00   49  \n",
       "1                                       Sharad Borle             0.00   49  \n",
       "2  Nicky Bull', 'Professor Yvonne Breyer', 'Dr Pr...             0.00   49  \n",
       "3                       Paul Bendich', 'Daniel Egger             0.25   49  \n",
       "4                          Charles Russell Severance             0.25   49  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = courses_data_cleaned.merge(desc_1, how='left', left_on='name', right_on='Course Title')\n",
    "merged_df = merged_df.merge(desc_2, how='left', left_on='name', right_on='course_title')\n",
    "merged_df = merged_df.merge(desc_3, how='left', left_on='name', right_on='Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 608654 entries, 0 to 608653\n",
      "Data columns (total 19 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   reviews               608591 non-null  object \n",
      " 1   reviewers             603398 non-null  object \n",
      " 2   rating                608654 non-null  int64  \n",
      " 3   name                  608654 non-null  object \n",
      " 4   institution           608654 non-null  object \n",
      " 5   Overall Ratings       608654 non-null  float64\n",
      " 6   Level                 608654 non-null  object \n",
      " 7   Duration              608654 non-null  int64  \n",
      " 8   Num of Reviews        608654 non-null  int64  \n",
      " 9   Skill gain            608654 non-null  object \n",
      " 10  Instructor            608654 non-null  object \n",
      " 11  Demeaned Rating       608654 non-null  float64\n",
      " 12  Fee                   608654 non-null  int64  \n",
      " 13  Course Title          329839 non-null  object \n",
      " 14  What you will learn   329839 non-null  object \n",
      " 15  course_title          417512 non-null  object \n",
      " 16  course_description_x  417512 non-null  object \n",
      " 17  Title                 192235 non-null  object \n",
      " 18  course_description_y  192235 non-null  object \n",
      "dtypes: float64(2), int64(4), object(13)\n",
      "memory usage: 92.9+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(['Course Title', 'course_title', 'Title'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309\n"
     ]
    }
   ],
   "source": [
    "unique_merged_df = merged_df.dropna(subset=['course_description_x', 'course_description_y', 'What you will learn'], how='all')\n",
    "unique_merged_df = unique_merged_df.drop_duplicates(subset='name')\n",
    "num_rows = unique_merged_df.shape[0]\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>reviewers</th>\n",
       "      <th>rating</th>\n",
       "      <th>name</th>\n",
       "      <th>institution</th>\n",
       "      <th>Overall Ratings</th>\n",
       "      <th>Level</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Num of Reviews</th>\n",
       "      <th>Skill gain</th>\n",
       "      <th>Instructor</th>\n",
       "      <th>Demeaned Rating</th>\n",
       "      <th>Fee</th>\n",
       "      <th>What you will learn</th>\n",
       "      <th>course_description_x</th>\n",
       "      <th>course_description_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The course was very useful, and it covered all...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Structuring Machine Learning Projects</td>\n",
       "      <td>DeepLearning.AI</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Beginner level</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>Decision-Making, Machine Learning, Deep Learni...</td>\n",
       "      <td>Younes Bensouda Mourri', 'Kian Katanforoosh', ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In the third course of the Deep Learning Speci...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very good refresher for Microsoft Excel</td>\n",
       "      <td>1017</td>\n",
       "      <td>5</td>\n",
       "      <td>Excel Skills for Business: Essentials</td>\n",
       "      <td>Macquarie University</td>\n",
       "      <td>4.9</td>\n",
       "      <td>Beginner level</td>\n",
       "      <td>26</td>\n",
       "      <td>47</td>\n",
       "      <td>Graphs, Spreadsheet, Microsoft Excel, Chart</td>\n",
       "      <td>Nicky Bull', 'Professor Yvonne Breyer', 'Dr Pr...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gain Skills to understand the fundamentals of ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I like the week 3 and week 4. Help me to revie...</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>Data Science Math Skills</td>\n",
       "      <td>Duke University</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Beginner level</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>\"Bayes Theorem\", Bayesian Probability, Probabi...</td>\n",
       "      <td>Paul Bendich', 'Daniel Egger</td>\n",
       "      <td>0.25</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This Specialization covers the concepts and to...</td>\n",
       "      <td>This Specialization covers the concepts and to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awesome!!!!!</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>Using Python to Access Web Data</td>\n",
       "      <td>University of Michigan</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Beginner level</td>\n",
       "      <td>19</td>\n",
       "      <td>43</td>\n",
       "      <td>Json, Xml, Python Programming, Web Scraping</td>\n",
       "      <td>Charles Russell Severance</td>\n",
       "      <td>0.25</td>\n",
       "      <td>49</td>\n",
       "      <td>Use regular expressions to extract data from s...</td>\n",
       "      <td>This course will show how one can treat the In...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>very good!</td>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>Introduction to Programming with MATLAB</td>\n",
       "      <td>Vanderbilt University</td>\n",
       "      <td>4.8</td>\n",
       "      <td>Beginner level</td>\n",
       "      <td>35</td>\n",
       "      <td>17</td>\n",
       "      <td>Computer Programming, Problem Solving, Matlab,...</td>\n",
       "      <td>Mike Fitzpatrick', 'Akos Ledeczi</td>\n",
       "      <td>0.25</td>\n",
       "      <td>49</td>\n",
       "      <td>You will learn fundamental computer programmin...</td>\n",
       "      <td>This course introduces you to sampling and exp...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews reviewers  rating  \\\n",
       "0  The course was very useful, and it covered all...         1       5   \n",
       "2            Very good refresher for Microsoft Excel      1017       5   \n",
       "3  I like the week 3 and week 4. Help me to revie...      2020       5   \n",
       "4                                       awesome!!!!!      2020       5   \n",
       "6                                         very good!      2020       5   \n",
       "\n",
       "                                      name             institution  \\\n",
       "0    Structuring Machine Learning Projects         DeepLearning.AI   \n",
       "2    Excel Skills for Business: Essentials    Macquarie University   \n",
       "3                 Data Science Math Skills         Duke University   \n",
       "4          Using Python to Access Web Data  University of Michigan   \n",
       "6  Introduction to Programming with MATLAB   Vanderbilt University   \n",
       "\n",
       "   Overall Ratings           Level  Duration  Num of Reviews  \\\n",
       "0              4.8  Beginner level         6              49   \n",
       "2              4.9  Beginner level        26              47   \n",
       "3              4.5  Beginner level        13              11   \n",
       "4              4.8  Beginner level        19              43   \n",
       "6              4.8  Beginner level        35              17   \n",
       "\n",
       "                                          Skill gain  \\\n",
       "0  Decision-Making, Machine Learning, Deep Learni...   \n",
       "2        Graphs, Spreadsheet, Microsoft Excel, Chart   \n",
       "3  \"Bayes Theorem\", Bayesian Probability, Probabi...   \n",
       "4        Json, Xml, Python Programming, Web Scraping   \n",
       "6  Computer Programming, Problem Solving, Matlab,...   \n",
       "\n",
       "                                          Instructor  Demeaned Rating  Fee  \\\n",
       "0  Younes Bensouda Mourri', 'Kian Katanforoosh', ...             0.00   49   \n",
       "2  Nicky Bull', 'Professor Yvonne Breyer', 'Dr Pr...             0.00   49   \n",
       "3                       Paul Bendich', 'Daniel Egger             0.25   49   \n",
       "4                          Charles Russell Severance             0.25   49   \n",
       "6                   Mike Fitzpatrick', 'Akos Ledeczi             0.25   49   \n",
       "\n",
       "                                 What you will learn  \\\n",
       "0                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  Use regular expressions to extract data from s...   \n",
       "6  You will learn fundamental computer programmin...   \n",
       "\n",
       "                                course_description_x  \\\n",
       "0  In the third course of the Deep Learning Speci...   \n",
       "2  Gain Skills to understand the fundamentals of ...   \n",
       "3  This Specialization covers the concepts and to...   \n",
       "4  This course will show how one can treat the In...   \n",
       "6  This course introduces you to sampling and exp...   \n",
       "\n",
       "                                course_description_y  \n",
       "0                                                NaN  \n",
       "2                                                NaN  \n",
       "3  This Specialization covers the concepts and to...  \n",
       "4                                                NaN  \n",
       "6                                                NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(row):\n",
    "\n",
    "    x = row['course_description_x'] if pd.notna(row['course_description_x']) else ''\n",
    "    y = row['course_description_y'] if pd.notna(row['course_description_y']) else ''\n",
    "    z = row['What you will learn'] if pd.notna(row['What you will learn']) else ''\n",
    "    \n",
    "    if x == y:\n",
    "        return x + ' ' + '\\n' + ' ' + z\n",
    "    elif x == z:\n",
    "        return x + ' ' + '\\n' + ' ' + y\n",
    "    elif y == z:\n",
    "        return y + ' ' + '\\n' + ' ' + x\n",
    "    else:\n",
    "        return x + ' ' + '\\n' + ' ' + y + ' ' + '\\n' + ' ' + z\n",
    "\n",
    "unique_merged_df['merged_description'] = unique_merged_df.apply(merge_columns, axis=1)\n",
    "unique_merged_df = unique_merged_df.reset_index(drop=True)\n",
    "unique_merged_df['course_id'] = unique_merged_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = unique_merged_df[['course_id', 'name', 'merged_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Quang\n",
      "[nltk_data]     Anh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Quang\n",
      "[nltk_data]     Anh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Quang\n",
      "[nltk_data]     Anh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "additional_stopwords = ['learn', 'course', 'students', 'introduction', 'understand', 'overview', \n",
    "    'concepts', 'designed', 'including', 'using', 'used', 'provides', \n",
    "    'students', 'aimed', 'teach', 'taught', 'include', 'includes', 'including', \n",
    "    'understanding', 'module', 'modules', 'cover', 'covers', 'covered', \n",
    "    'introduce', 'introduces', 'introduced', 'overview', 'knowledge', 'skills', \n",
    "    'learning', 'lecture', 'lectures', 'topic', 'topics', 'area', 'areas', \n",
    "    'focus', 'focuses', 'focused', 'approach', 'approaches', 'principles', \n",
    "    'principle', 'content', 'contents', 'key', 'features', 'feature', \n",
    "    'element', 'elements', 'basis', 'basic', 'basics', 'foundation', 'foundations',\n",
    "    'beginner', 'intermediate', 'advanced', 'level', 'levels', 'intended',\n",
    "    'objective', 'objectives', 'goal', 'goals', 'outcome', 'outcomes', 'aim', 'aims',\n",
    "    'gain', 'gains', 'apply', 'effective', 'efficient', 'specialize', 'specialization',\n",
    "    'specialise', 'certificate', 'certification', 'you', 'will', 'need',\n",
    "    'want', 'have', 'able', 'well', 'one', 'two', 'three', 'first', 'second',\n",
    "    'third', 'success', 'successful', 'sucessfully', 'like', 'many' , 'much', 'also', 'use', 'uses', 'used',\n",
    "    'work', 'works', 'working', 'workshop', 'workshops', 'provide', 'provides', 'provided']\n",
    "\n",
    "all_stop_words = stopwords.words('english') + additional_stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Preprocessing function with N-gram consideration\n",
    "def preprocess_text(text, lemmatizer=WordNetLemmatizer(), stop_words=all_stop_words):\n",
    "    text = text.lower()  # Lowercasing\n",
    "    # text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    alphabetic_tokens = [token for token in tokens if re.match(\"^[a-zA-Z]+$\", token)] # check for alphabetic tokens\n",
    "    filtered_tokens = [word for word in alphabetic_tokens if word not in stop_words]  # Remove stopwords\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]  # Lemmatization\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If 'courses' might be a slice of another DataFrame, explicitly create a copy to work on:\n",
    "courses = courses.copy()\n",
    "\n",
    "# Now, safely apply the preprocessing and assign to the 'processed_description' column\n",
    "courses['processed_description'] = courses['merged_description'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Assuming courses['processed_description'] is your preprocessed text\n",
    "embeddings = model.encode(courses['processed_description'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Let's assume we want to reduce to 50 dimensions\n",
    "pca = PCA(n_components=50)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity(reduced_embeddings)\n",
    "\n",
    "# Normalize Euclidean distances to be between 0 and 1\n",
    "max_dist = np.max(euclidean_distances(reduced_embeddings))\n",
    "normalized_euclidean_sim = 1 - (euclidean_distances(reduced_embeddings) / max_dist)\n",
    "\n",
    "# Hybrid similarity measure: average of cosine and normalized Euclidean\n",
    "hybrid_sim = (cosine_sim + normalized_euclidean_sim) / 2\n",
    "\n",
    "# Now you can use hybrid_sim for recommendations similar to previous examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier courses (wrong descriptions)\n",
    "\n",
    "courses = courses.drop(91)\n",
    "courses = courses.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "courses['course_id'] = courses.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction to Systematic Review and Meta-Analysis\n",
      "     course_id                                              name  \\\n",
      "106        106                          AI for Medical Diagnosis   \n",
      "209        209                                     Dentistry 101   \n",
      "160        160  Introduction to Chemistry:  Reactions and Ratios   \n",
      "251        251                        Getting Started with Azure   \n",
      "198        198                       Data-driven Decision Making   \n",
      "\n",
      "                                    merged_description  \n",
      "106   \\n AI is transforming the practice of medicin...  \n",
      "209   \\n Explore the role of dentists as healthcare...  \n",
      "160   \\n The focus and themes of the Introduction t...  \n",
      "251   \\n Navigating the Azure Portal and Creating C...  \n",
      "198  One of the most important skills of successful...  \n"
     ]
    }
   ],
   "source": [
    "def recommend_courses(course_id, similarity_matrix, top_n=5):\n",
    "    course_index = courses[courses['course_id'] == course_id].index[0]\n",
    "    similarity_scores = list(enumerate(similarity_matrix[course_index]))\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "    recommended_course_indices = [i[0] for i in similarity_scores[1:top_n+1]]\n",
    "    return courses.iloc[recommended_course_indices]\n",
    "\n",
    "# Example usage: Recommend courses similar to course_id 1 based on hybrid similarity\n",
    "course_number = random.randint(0, len(courses))\n",
    "print(courses['name'][course_number])\n",
    "similar_courses = recommend_courses(course_number, hybrid_sim, )\n",
    "print(similar_courses[['course_id', 'name', 'merged_description']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Science Math Skills\n",
      "                                                  name  course_id\n",
      "46                              Tools for Data Science         46\n",
      "55              Introduction to Data Science in Python         55\n",
      "58                                SQL for Data Science         58\n",
      "130  Biology Meets Programming: Bioinformatics for ...        130\n",
      "213                      Applied Data Science Capstone        213\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))  # Considering unigrams, bigrams, and trigrams\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(courses['processed_description'])\n",
    "\n",
    "# Cosine Similarity\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Example: Find courses similar to course_id 1\n",
    "def get_similar_courses(course_id, similarity_matrix, courses_df):\n",
    "    # Get course index\n",
    "    course_idx = courses_df.index[courses_df['course_id'] == course_id].tolist()[0]\n",
    "    # Get similarity scores and pair with course index\n",
    "    similarity_scores = list(enumerate(similarity_matrix[course_idx]))\n",
    "    # Sort the courses based on similarity scores\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores of the 5 most similar courses (excluding itself)\n",
    "    most_similar_courses = similarity_scores[1:6]\n",
    "    # Get course ids\n",
    "    similar_course_ids = [courses_df.iloc[i[0]]['course_id'] for i in most_similar_courses]\n",
    "    return similar_course_ids\n",
    "\n",
    "# Get similar courses to a given course_id\n",
    "\n",
    "course_number = random.randint(0, len(courses))\n",
    "print(courses['name'][course_number])\n",
    "similar_courses = get_similar_courses(course_number, cosine_sim_matrix, courses)\n",
    "print(courses.loc[courses['course_id'].isin(similar_courses), ['name', 'course_id']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
